{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "file_path (str): path to input file\n",
    "\n",
    "Output:\n",
    "words (3d numpy array): list of sentences;\n",
    "                        each sentence: list of words coupled with corresponding state \n",
    "                        [[[word, state],[word_2, state_2],...],[sentence_2],...]\n",
    "labels (list): list of tags/states including START and STOP\n",
    "\n",
    "Function:\n",
    "Takes input file, outputs the words arranged with corresponding states and a list of all possible tags/states\n",
    "'''\n",
    "def train(file_path):\n",
    "    with open (file_path, 'r', encoding=\"utf-8\") as f: \n",
    "        lines = f.readlines()\n",
    "        words = []\n",
    "        labels = []\n",
    "\n",
    "        temp = []\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                l_split = l.strip().split(\" \")\n",
    "                te = \" \".join(l_split[:-1])\n",
    "                la = l_split[-1]\n",
    "                temp.append([te, la])\n",
    "                labels.append(l_split[-1])\n",
    "            else: \n",
    "                words.append(temp)\n",
    "                temp = []\n",
    "\n",
    "    l_labels = [\"START\"]\n",
    "    for i in labels:\n",
    "        if i in l_labels:\n",
    "            continue\n",
    "        else:\n",
    "            l_labels.append(i)\n",
    "    l_labels.append(\"STOP\")\n",
    "    return words, l_labels\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "u (str)： initial state/tag\n",
    "v (str)： final state/tag\n",
    "words (list)：list of words\n",
    "\n",
    "Output:\n",
    "transition_probability (float): transition probability from state u to state v\n",
    "\n",
    "Function:\n",
    "Takes initial state u and final state v, calculates the transition probability from state u to state v\n",
    "'''\n",
    "def transition(u,v,words):\n",
    "    count_u = 0\n",
    "    count_u_v = 0\n",
    "    for sentence in words:\n",
    "        if u == \"START\":\n",
    "            count_u = len(words)\n",
    "            if sentence[0][1] == v:\n",
    "                count_u_v +=1\n",
    "        elif v == \"STOP\":\n",
    "            for i in range(len(sentence)):\n",
    "                if sentence[i][1] == u:\n",
    "                    count_u += 1\n",
    "                    if i+1==len(sentence):\n",
    "                        count_u_v += 1\n",
    "        else:\n",
    "            for i in range(len(sentence)):\n",
    "                if sentence[i][1] == u:\n",
    "                    count_u += 1\n",
    "                    if i+1!=len(sentence) and sentence[i+1][1] == v:\n",
    "                        count_u_v += 1\n",
    "    return count_u_v/count_u\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "words (list)：list of words\n",
    "l_labels (list): list of all possible states\n",
    "\n",
    "Output:\n",
    "transition_table (2d numpy array): transition table for given training set \n",
    "\n",
    "Function:\n",
    "Takes list of all possible states and outputs the transition table for given training set\n",
    "'''\n",
    "def generate_transition_table(words, l_labels):\n",
    "    transition_table = np.zeros([len(l_labels[:-1]),len(l_labels[1:])])\n",
    "    for row_idx, label_row in tqdm(enumerate(l_labels[:-1])):\n",
    "        for col_idx, label_col in enumerate(l_labels[1:]):\n",
    "            transition_table[row_idx][col_idx] = transition(label_row, label_col,words)\n",
    "    return transition_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "x (list): list of words with corresponding tag in y\n",
    "y (list): list of tags with corresponding word in x\n",
    "list_of_tags (list): all tags that could be formed\n",
    "\n",
    "Output:\n",
    "tag_word_table (dictionary): dictionary with keys being tags and values being the associated words\n",
    "\n",
    "Function:\n",
    "Format data into a dictionary. Length of x and y must be the same.\n",
    "'''\n",
    "def generate_table(x, y):\n",
    "    if len(x)!=len(y):\n",
    "        print(\"ERROR: difference in length between data and tag\")\n",
    "        return None\n",
    "    \n",
    "    tag_word_table = {}\n",
    "    for tag, word in tqdm(zip(y,x)):\n",
    "        if tag in tag_word_table:\n",
    "            tag_word_table[tag].append(word)\n",
    "        else:\n",
    "            tag_word_table[tag] = [word]\n",
    "            \n",
    "    return tag_word_table\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "x (str): word to be queried\n",
    "y (str): label to be queried\n",
    "tag_word_table (dictionary): data in table form\n",
    "k (float): number of occurences #UNK# is found\n",
    "\n",
    "Output:\n",
    "probability (float): probability of generating x from y based on tag_word_table\n",
    "\n",
    "Function:\n",
    "Calculated emission probability\n",
    "'''\n",
    "def emission(x, y, tag_word_table, k = 0.5):\n",
    "    word_list = tag_word_table[y]\n",
    "    if x == \"#UNK#\":\n",
    "        emission_count = k\n",
    "    else:\n",
    "        emission_count = word_list.count(x)\n",
    "    ycount = len(word_list)\n",
    "    return emission_count / (ycount + k)\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "x (list): list of words\n",
    "\n",
    "Output:\n",
    "word_list (list): list of unique words\n",
    "\n",
    "Function:\n",
    "Generates a list of all unique words in x\n",
    "'''\n",
    "def generate_word_list(x):\n",
    "    word_list = []\n",
    "    for i in tqdm(x):\n",
    "        if i not in word_list:\n",
    "            word_list.append(i)\n",
    "    word_list.append(\"#UNK#\")\n",
    "    return word_list\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "list_x (list): list of words\n",
    "word_list (list): list of unique words\n",
    "tag_word_table (dictionary): dictionary form of the data\n",
    "\n",
    "Output:\n",
    "emission_table (numpy array): 2D numpy array with row each row representing a word and each column a tag\n",
    "\n",
    "Function:\n",
    "Generates the emission table, where each word has its emission value stored in a numpy array\n",
    "'''\n",
    "def generate_emission_table(list_x, word_list, tag_word_table):\n",
    "    # Each row is the word\n",
    "    # Each column is the tag in tag_word_table\n",
    "    emission_table = np.zeros([len(word_list), len(tag_word_table.keys())])\n",
    "    \n",
    "    tags = tag_word_table.keys()\n",
    "    \n",
    "    for ind_x,x in tqdm(enumerate(word_list)):\n",
    "        for ind_y, y in enumerate(tags):\n",
    "            em = emission(x,y,tag_word_table)\n",
    "            emission_table[ind_x,ind_y] = em\n",
    "    return emission_table\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "input_file (str): path to input file\n",
    "\n",
    "Output:\n",
    "x_list (list): list of words\n",
    "y_list (list): list of tags\n",
    "\n",
    "Function:\n",
    "Cleans the input file, as some lines have spaces within the word section. This function only takes the last\n",
    "word delimited by spaces as the tag then recombines all words delimited by space in front to form the actual\n",
    "word. Then returns the data as a list of word_list and tag_list.\n",
    "'''\n",
    "def clean(input_file):\n",
    "    inp_f = open(input_file, \"r\", encoding=\"utf-8\")\n",
    "    lines = inp_f.readlines()\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for ind, l in tqdm(enumerate(lines)):\n",
    "        words = l.split(\" \")\n",
    "        if len(words)>2:\n",
    "            tag = words[-1].strip(\"\\n\")\n",
    "            act_word = \" \".join(words[:-1])\n",
    "            x_list.append(act_word)\n",
    "            y_list.append(tag)\n",
    "        elif len(words)==2:\n",
    "            x_list.append(words[0])\n",
    "            y_list.append(words[1].strip(\"\\n\"))\n",
    "        elif len(words)==1:\n",
    "            continue\n",
    "        else:\n",
    "            print(words)\n",
    "            print(str(ind) + \"training data has no label\")\n",
    "            print(\"data is: \" + words[0])\n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input:\n",
    "prev_pi_vals (len_statesx3x2 numpy array): previous pi values\n",
    "len_states (int): num of possible states\n",
    "tr_arr (2d numpy arr): transition table\n",
    "em_arr (2d numpy arr): emission table\n",
    "curr_state (int): current state index in the table\n",
    "word_idx (int): index of current word in word_list\n",
    "\n",
    "Output:\n",
    "top3 (3x2 numpy array): array of the top 3 values with the 1st value being the highest\n",
    "\n",
    "Function:\n",
    "Finds the top 3 probabilities and their parents\n",
    "'''\n",
    "def find_top3(prev_pi_vals, len_states, tr_arr, em_arr, curr_state, word_idx):\n",
    "    # Initialize\n",
    "    temp_vals = []\n",
    "\n",
    "    for u_idx, state in enumerate(prev_pi_vals):\n",
    "        for pi_val, parent_idx in state:\n",
    "            prev_state = u_idx\n",
    "            tr_val = tr_arr[prev_state+1][curr_state]\n",
    "            em_val = em_arr[word_idx][curr_state]\n",
    "            if tr_val == 0 or em_val == 0:\n",
    "                new_pi_val = float('-inf')\n",
    "            else:                        \n",
    "                new_pi_val = pi_val + np.log(tr_val) + np.log(em_val)\n",
    "            temp_vals.append([new_pi_val, prev_state])\n",
    "            \n",
    "    # Sort the values in descending order by using the first value of the list (pi_val)\n",
    "    if np.max(temp_vals) == float(\"-inf\"):\n",
    "        top3 = np.array([[float(\"-inf\"), 7], [float(\"-inf\"), 7], [float(\"-inf\"), 7]])\n",
    "    else:\n",
    "        sorted_vals = sorted(temp_vals, key = lambda x: x[0], reverse=True)\n",
    "        top3 = np.array(sorted_vals[:3])\n",
    "\n",
    "    return top3\n",
    "\n",
    "'''\n",
    "Input:\n",
    "parent_idx (int): index of parent of STOP\n",
    "n (list): input sentence\n",
    "pi_arr (2d numpy arr): table of pi values\n",
    "all_states (list): list of all the tags not including START and STOP\n",
    "paths_table (nested list): table containing all the paths of previous iterations for each node\n",
    "\n",
    "Output:\n",
    "pred_vals (list): predicted values\n",
    "paths_table (list of list): updated path table for this iteration\n",
    "\n",
    "Function:\n",
    "Traceback using the parent values\n",
    "'''\n",
    "                                      \n",
    "def traceback(parent_idx, n, pi_arr, all_states, paths_table):\n",
    "    pred_vals = []\n",
    "    path = ['STOP']\n",
    "    for curr_idx in range(len(n)-1,0,-1):\n",
    "        best_idx = paths_table[curr_idx][parent_idx].count(path)\n",
    "        paths_table[curr_idx][parent_idx].append(path)\n",
    "        \n",
    "        next_path = path + [all_states[parent_idx]]\n",
    "        path = next_path\n",
    "        pred_vals.append(all_states[parent_idx])\n",
    "\n",
    "        _, parent_idx = pi_arr[parent_idx, curr_idx, best_idx, :]\n",
    "        parent_idx = int(parent_idx)\n",
    "        \n",
    "    pred_vals.append(all_states[parent_idx])\n",
    "    pred_vals.reverse()\n",
    "    return pred_vals, paths_table\n",
    "\n",
    "'''\n",
    "Input:\n",
    "n (list of str): sentence split into list by spaces\n",
    "l_labels (list of str): list of tags\n",
    "tr_arr (2d numpy arr): transmission table tr_arr[initial state, final state]\n",
    "em_arr (2d numpy arr): emission table em_arr[word, state]\n",
    "word_list (list of str): list of all words in the training set\n",
    "\n",
    "Output:\n",
    "pred_tag (list of string): list of predicted tags\n",
    "\n",
    "Function:\n",
    "Does modified viterbi algorithm to find the top 3 best path\n",
    "'''\n",
    "\n",
    "def viterbi_top3(n, l_labels, tr_arr, em_arr, word_list):\n",
    "    args = []\n",
    "    all_states = l_labels[1:-1]\n",
    "    len_states = len(all_states)\n",
    "    \n",
    "    # Set up pi array with each state containing top 3 pi states and within each state contains the [pi_prob, parent_idx]\n",
    "    pi_arr = np.zeros([len_states,len(n),3,2])\n",
    "    \n",
    "    # going through the table\n",
    "    for j in range(len(n)):\n",
    "        \n",
    "        curr_word = n[j]\n",
    "        if curr_word not in word_list:\n",
    "            curr_word = \"#UNK#\"\n",
    "        word_idx = word_list.index(curr_word)\n",
    "        \n",
    "        # First column\n",
    "        if j == 0:\n",
    "            for curr_state in range(len_states):\n",
    "                if tr_arr[0][curr_state] == 0 or em_arr[word_idx][curr_state] == 0:\n",
    "                    pi_val = float('-inf')\n",
    "                else:\n",
    "                    pi_val = np.log(tr_arr[0][curr_state]) + np.log(em_arr[word_idx][curr_state])\n",
    "                pi_arr[curr_state,j] =  np.array([[pi_val, 0], [pi_val, 0], [pi_val, 0]])\n",
    "                \n",
    "        # Second column\n",
    "        elif j == 1:\n",
    "            for curr_state in range(len_states):\n",
    "                temp_vals = []\n",
    "                prev_pi_vals = pi_arr[:,j-1,:,:]\n",
    "                for u_idx, state in enumerate(prev_pi_vals):\n",
    "                    pi_val, parent_idx = state[0,:]\n",
    "                    prev_state = u_idx\n",
    "                    if tr_arr[prev_state+1][curr_state] == 0 or em_arr[word_idx][curr_state] == 0:\n",
    "                        new_pi_val = float('-inf')\n",
    "                    else:\n",
    "                        new_pi_val = pi_val + np.log(tr_arr[prev_state+1][curr_state]) + np.log(em_arr[word_idx][curr_state])\n",
    "                    temp_vals.append([new_pi_val, prev_state])\n",
    "                # Sort the values in descending order by using the first value of the list (pi_val)\n",
    "                if np.max(temp_vals) == float(\"-inf\"):\n",
    "                    pi_arr[curr_state,j] = np.array([[float(\"-inf\"), 7], [float(\"-inf\"), 7], [float(\"-inf\"), 7]])\n",
    "                else:\n",
    "                    sorted_vals = sorted(temp_vals, key = lambda x: x[0], reverse=True)\n",
    "                    pi_arr[curr_state,j] = np.array(sorted_vals[:3])\n",
    "    \n",
    "        # All other columns\n",
    "        elif j > 1:\n",
    "            for curr_state in range(len_states):\n",
    "                prev_pi_vals = pi_arr[:,j-1,:,:]\n",
    "                top3 = find_top3(prev_pi_vals, len_states, tr_arr, em_arr, curr_state, word_idx)\n",
    "                pi_arr[curr_state,j] = top3\n",
    "    \n",
    "    temp_vals = []\n",
    "    last_pi_vals = pi_arr[:,len(n)-1,:,:]\n",
    "    for u_idx, state in enumerate(last_pi_vals):\n",
    "        for pi_val, parent_idx in state:\n",
    "            prev_state = u_idx\n",
    "            tr_val = tr_arr[prev_state+1][-1]\n",
    "            if tr_val == 0:\n",
    "                new_pi_val = float('-inf')\n",
    "            else:\n",
    "                new_pi_val = pi_val + np.log(tr_val)\n",
    "            temp_vals.append([new_pi_val, prev_state])\n",
    "    # Sort the values in descending order by using the first value of the list (pi_val)\n",
    "    if np.max(temp_vals) == float(\"-inf\"):\n",
    "        top3 = np.array([[float(\"-inf\"), 7], [float(\"-inf\"), 7], [float(\"-inf\"), 7]])\n",
    "    else:\n",
    "        sorted_vals = sorted(temp_vals, key = lambda x: x[0], reverse=True)\n",
    "        top3 = np.array(sorted_vals[:3])\n",
    "    \n",
    "    # paths_table contains all the paths that visited that node for each node in the graph\n",
    "    # the path at index j and with state_ind state is paths_table[j][state] \n",
    "    paths_table = []\n",
    "    path = []\n",
    "    for j in range(len(n)):\n",
    "        temp_paths = []\n",
    "        for state in range(len_states):\n",
    "            temp_paths.append([])\n",
    "        paths_table.append(temp_paths)\n",
    "    \n",
    "    pred_vals_best, paths_table = traceback(int(top3[0,1]), n, pi_arr, all_states, paths_table)\n",
    "    pred_vals_2ndbest, paths_table = traceback(int(top3[1,1]), n, pi_arr, all_states, paths_table)\n",
    "    pred_vals_3rdbest, paths_table = traceback(int(top3[2,1]), n, pi_arr, all_states, paths_table)\n",
    "    \n",
    "    return pred_vals_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "input_file_path (str): path to input file\n",
    "output_file_path (str): path to output file\n",
    "labels (list): list of all possible tags/states\n",
    "transition_table (2d numpy array): transition table calculated from training set\n",
    "emission_table (2d numpy array): emission table calculated from training set\n",
    "word_list (list): list of words\n",
    "\n",
    "Output:\n",
    " - : write predicted results to output file\n",
    "\n",
    "Function:\n",
    "Takes input file, for every sentence in input file, generates the predicted tag/state for each word in sentence by running\n",
    "viterbi() and write to output file \n",
    "'''\n",
    "def generate_prediction_top3(input_file_path, output_file_path, labels, transition_table, emission_table, word_list):\n",
    "    with open (input_file_path, 'r', encoding=\"utf-8\") as dev_in: \n",
    "        lines = dev_in.readlines()\n",
    "        words = []\n",
    "\n",
    "        temp = []\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                l_split = l.strip().split(\" \")\n",
    "                la = l_split[-1]\n",
    "                temp.append(la)\n",
    "            else: \n",
    "                words.append(temp)\n",
    "                temp = []\n",
    "\n",
    "        words_copy = copy.deepcopy(words)\n",
    "        labels_out = []\n",
    "        for sentence in tqdm(words):\n",
    "            label_out = viterbi_top3(sentence, labels, transition_table, emission_table, word_list)\n",
    "            labels_out.append(label_out)\n",
    "            \n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for i in tqdm(range(len(words_copy))):\n",
    "            for j in range(len(words_copy[i])):\n",
    "                output = words_copy[i][j] + \" \" + labels_out[i][j] + \"\\n\"\n",
    "                fout.write(output)\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:04,  4.44it/s]\n",
      "189291it [00:00, 1636201.76it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 181628/181628 [00:04<00:00, 36371.29it/s]\n",
      "181628it [00:00, 3433834.48it/s]\n",
      "18213it [01:42, 177.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN donez\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "words_en, labels_en = train(\"EN/train\")\n",
    "\n",
    "transition_table_en = generate_transition_table(words_en, labels_en)\n",
    "\n",
    "x_en, y_en = clean(\"EN/train\")\n",
    "word_list_en = generate_word_list(x_en)\n",
    "tag_word_table_en = generate_table(x_en, y_en)\n",
    "emission_table_en = generate_emission_table(x_en, word_list_en, tag_word_table_en)\n",
    "\n",
    "print(\"EN donez\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1094/1094 [01:53<00:00,  9.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1094/1094 [00:00<00:00, 91602.82it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_prediction_top3(\"EN/dev.in\", \"EN/dev.p4.out\", labels_en, transition_table_en, emission_table_en, word_list_en)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
