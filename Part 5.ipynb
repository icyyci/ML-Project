{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "'''\n",
    "Inputs:\n",
    "file_path (str): path to input file\n",
    "\n",
    "Output:\n",
    "words (3d numpy array): list of sentences;\n",
    "                        each sentence: list of words coupled with corresponding state \n",
    "                        [[[word, state],[word_2, state_2],...],[sentence_2],...]\n",
    "labels (list): list of tags/states including START and STOP\n",
    "\n",
    "Function:\n",
    "Takes input file, outputs the words arranged with corresponding states and a list of all possible tags/states\n",
    "'''\n",
    "def train(file_path):\n",
    "    with open (file_path, 'r', encoding=\"utf-8\") as f: \n",
    "        lines = f.readlines()\n",
    "        words = []\n",
    "        labels = []\n",
    "\n",
    "        temp = []\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                l_split = l.strip().split(\" \")\n",
    "                te = \" \".join(l_split[:-1])\n",
    "                la = l_split[-1]\n",
    "                temp.append([te, la])\n",
    "                labels.append(l_split[-1])\n",
    "            else: \n",
    "                words.append(temp)\n",
    "                temp = []\n",
    "\n",
    "    l_labels = [\"START\"]\n",
    "    for i in labels:\n",
    "        if i in l_labels:\n",
    "            continue\n",
    "        else:\n",
    "            l_labels.append(i)\n",
    "    l_labels.append(\"STOP\")\n",
    "    return words, l_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "'''\n",
    "Inputs:\n",
    "input_file_path (str): path to input file\n",
    "output_file_path (str): path to output file\n",
    "labels (list): list of all possible tags/states\n",
    "transition_table (2d numpy array): transition table calculated from training set\n",
    "emission_table (2d numpy array): emission table calculated from training set\n",
    "word_list (list): list of words\n",
    "\n",
    "Output:\n",
    " - : write predicted results to output file\n",
    "\n",
    "Function:\n",
    "Takes input file, for every sentence in input file, generates the predicted tag/state for each word in sentence by running\n",
    "viterbi() and write to output file \n",
    "'''\n",
    "def generate_prediction(input_file_path, output_file_path, labels, transition_table, emission_table, word_list):\n",
    "    with open (input_file_path, 'r', encoding=\"utf-8\") as dev_in: \n",
    "        lines = dev_in.readlines()\n",
    "        words = []\n",
    "\n",
    "        temp = []\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                l_split = l.strip().split(\" \")\n",
    "                la = l_split[-1]\n",
    "                temp.append(la)\n",
    "            else: \n",
    "                words.append(temp)\n",
    "                temp = []\n",
    "\n",
    "        words_copy = copy.deepcopy(words)\n",
    "        labels_out = []\n",
    "        for sentence in tqdm(words):\n",
    "            label_out = viterbi(sentence, labels, transition_table, emission_table, word_list )\n",
    "            labels_out.append(label_out)\n",
    "            \n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for i in tqdm(range(len(words_copy))):\n",
    "            for j in range(len(words_copy[i])):\n",
    "                output = words_copy[i][j] + \" \" + labels_out[i][j] + \"\\n\"\n",
    "                fout.write(output)\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "u (str)： initial state/tag\n",
    "v (str)： final state/tag\n",
    "words (list)：list of words\n",
    "\n",
    "Output:\n",
    "transition_probability (float): transition probability from state u to state v\n",
    "\n",
    "Function:\n",
    "Takes initial state u and final state v, calculates the transition probability from state u to state v\n",
    "'''\n",
    "def transition(u,v,words):\n",
    "    count_u = 0\n",
    "    count_u_v = 0\n",
    "    for sentence in words:\n",
    "        if u == \"START\":\n",
    "            count_u = len(words)\n",
    "            if sentence[0][1] == v:\n",
    "                count_u_v +=1\n",
    "        elif v == \"STOP\":\n",
    "            for i in range(len(sentence)):\n",
    "                if sentence[i][1] == u:\n",
    "                    count_u += 1\n",
    "                    if i+1 == len(sentence):\n",
    "                        count_u_v += 1\n",
    "        else:\n",
    "            for i in range(len(sentence)):\n",
    "                if sentence[i][1] == u:\n",
    "                    count_u += 1\n",
    "                    if i+1!=len(sentence) and sentence[i+1][1] == v:\n",
    "                        count_u_v += 1\n",
    "    return count_u_v/count_u\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "words (list)：list of words\n",
    "l_labels (list): list of all possible states\n",
    "\n",
    "Output:\n",
    "transition_table (2d numpy array): transition table for given training set \n",
    "\n",
    "Function:\n",
    "Takes list of all possible states and outputs the transition table for given training set\n",
    "'''\n",
    "def generate_transition_table(words, l_labels):\n",
    "    transition_table = np.zeros([len(l_labels[:-1]),len(l_labels[1:])])\n",
    "    for row_idx, label_row in tqdm(enumerate(l_labels[:-1])):\n",
    "        for col_idx, label_col in enumerate(l_labels[1:]):\n",
    "            transition_table[row_idx][col_idx] = transition(label_row, label_col,words)\n",
    "    return transition_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get YX's emission table\n",
    "'''\n",
    "Inputs:\n",
    "x (list): list of words with corresponding tag in y\n",
    "y (list): list of tags with corresponding word in x\n",
    "list_of_tags (list): all tags that could be formed\n",
    "\n",
    "Output:\n",
    "tag_word_table (dictionary): dictionary with keys being tags and values being the associated words\n",
    "\n",
    "Function:\n",
    "Format data into a dictionary. Length of x and y must be the same.\n",
    "'''\n",
    "def generate_table(x, y):\n",
    "    if len(x)!=len(y):\n",
    "        print(\"ERROR: difference in length between data and tag\")\n",
    "        return None\n",
    "    \n",
    "    tag_word_table = {}\n",
    "    for tag, word in tqdm(zip(y,x)):\n",
    "        if tag in tag_word_table:\n",
    "            tag_word_table[tag].append(word)\n",
    "        else:\n",
    "            tag_word_table[tag] = [word]\n",
    "            \n",
    "    return tag_word_table\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "x (str): word to be queried\n",
    "y (str): label to be queried\n",
    "tag_word_table (dictionary): data in table form\n",
    "k (float): number of occurences #UNK# is found\n",
    "\n",
    "Output:\n",
    "probability (float): probability of generating x from y based on tag_word_table\n",
    "\n",
    "Function:\n",
    "Calculated emission probability\n",
    "'''\n",
    "def emission(x, y, tag_word_table, k = 0.5):\n",
    "    word_list = tag_word_table[y]\n",
    "    if x == \"#UNK#\":\n",
    "        emission_count = k\n",
    "    else:\n",
    "        emission_count = word_list.count(x)\n",
    "    ycount = len(word_list)\n",
    "    return emission_count / (ycount + k)\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "x (list): list of words\n",
    "\n",
    "Output:\n",
    "word_list (list): list of unique words\n",
    "\n",
    "Function:\n",
    "Generates a list of all unique words in x\n",
    "'''\n",
    "def generate_word_list(x):\n",
    "    word_list = []\n",
    "    for i in tqdm(x):\n",
    "        if i not in word_list:\n",
    "            word_list.append(i)\n",
    "    word_list.append(\"#UNK#\")\n",
    "    return word_list\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "list_x (list): list of words\n",
    "word_list (list): list of unique words\n",
    "tag_word_table (dictionary): dictionary form of the data\n",
    "\n",
    "Output:\n",
    "emission_table (numpy array): 2D numpy array with row each row representing a word and each column a tag\n",
    "\n",
    "Function:\n",
    "Generates the emission table, where each word has its emission value stored in a numpy array\n",
    "'''\n",
    "def generate_emission_table(word_list, tag_word_table):\n",
    "    # Each row is the word\n",
    "    # Each column is the tag in tag_word_table\n",
    "    emission_table = np.zeros([len(word_list), len(tag_word_table.keys())])\n",
    "    \n",
    "    tags = tag_word_table.keys()\n",
    "    \n",
    "    for ind_x,x in tqdm(enumerate(word_list)):\n",
    "        for ind_y, y in enumerate(tags):\n",
    "            em = emission(x,y,tag_word_table)\n",
    "            emission_table[ind_x,ind_y] = em\n",
    "    return emission_table\n",
    "\n",
    "'''\n",
    "Inputs:\n",
    "input_file (str): path to input file\n",
    "\n",
    "Output:\n",
    "x_list (list): list of words\n",
    "y_list (list): list of tags\n",
    "\n",
    "Function:\n",
    "Cleans the input file, as some lines have spaces within the word section. This function only takes the last\n",
    "word delimited by spaces as the tag then recombines all words delimited by space in front to form the actual\n",
    "word. Then returns the data as a list of word_list and tag_list.\n",
    "'''\n",
    "def clean(input_file):\n",
    "    inp_f = open(input_file, \"r\", encoding=\"utf-8\")\n",
    "    lines = inp_f.readlines()\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    for ind, l in tqdm(enumerate(lines)):\n",
    "        words = l.split(\" \")\n",
    "        if len(words)>2:\n",
    "            tag = words[-1].strip(\"\\n\")\n",
    "            act_word = \" \".join(words[:-1])\n",
    "            x_list.append(act_word)\n",
    "            y_list.append(tag)\n",
    "        elif len(words)==2:\n",
    "            x_list.append(words[0])\n",
    "            y_list.append(words[1].strip(\"\\n\"))\n",
    "        elif len(words)==1:\n",
    "            continue\n",
    "        else:\n",
    "            print(words)\n",
    "            print(str(ind) + \"training data has no label\")\n",
    "            print(\"data is: \" + words[0])\n",
    "    return x_list, y_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "n (list)： list of input words to be predicted\n",
    "l_labels (str)： list of all possible states\n",
    "tr_arr (2d numpy array): transition table\n",
    "em_arr (2d numpy array): emission table\n",
    "word_list (list): list of words\n",
    "\n",
    "Output:\n",
    "args (list): list of predicted states \n",
    "\n",
    "Function:\n",
    "Takes input sentence of n words, outputs list of n predicted tags/states corresponding to input words \n",
    "'''\n",
    "def viterbi(n, l_labels, tr_arr, em_arr, word_list):\n",
    "        \n",
    "    # Initializoing pi array \n",
    "    pi_arr = np.zeros([len(l_labels[1:-1]),len(n)+2])\n",
    "    \n",
    "    ### Forwarding Algo\n",
    "    \n",
    "    # Iterating through the columns of the pi array\n",
    "    for j in range(len(n)+2):\n",
    "        \n",
    "        # column 0: START nodes, assign value 1 to all nodes in column 0\n",
    "        if j == 0:\n",
    "            for i in range(len(pi_arr)):\n",
    "                pi_arr[i][j] = 1\n",
    "                \n",
    "        # column 1: nodes right after START, pi value = 1(pi value of START) * transition_prob * emission_ prob       \n",
    "        elif j == 1:\n",
    "            if n[j-1] not in word_list:\n",
    "                n[j-1] = \"#UNK#\"\n",
    "            for u_idx, u in enumerate((l_labels[1:-1])):\n",
    "                if tr_arr[0][u_idx] == 0 or em_arr[word_list.index(n[j-1])][u_idx] == 0:\n",
    "                    pi_arr[u_idx][j] = float('-inf')\n",
    "                else:\n",
    "                    pi_arr[u_idx][j] = np.log(tr_arr[0][u_idx]) + np.log(em_arr[word_list.index(n[j-1])][u_idx])\n",
    "        \n",
    "        # columns 2 to n : pi value = max(pi value of previous nodes * transition_prob * emission_ prob)\n",
    "        elif j > 1 and j < len(n)+1:\n",
    "            \n",
    "            # n[j-1]: current word in sentence, if not found in word_list, replace with \"#UNK#\"\n",
    "            if n[j-1] not in word_list:\n",
    "                n[j-1] = \"#UNK#\"\n",
    "            \n",
    "            # Iterating through column v: current column\n",
    "            for u_idx, u in enumerate((l_labels[1:-1])): # v\n",
    "                \n",
    "                # array to store temp scores\n",
    "                pi_temp = []\n",
    "                \n",
    "                # Iterating through column u: previous column\n",
    "                for u_idx_temp, u_temp in enumerate((l_labels[1:-1])): # u\n",
    "                    if tr_arr[u_idx_temp+1][u_idx] == 0 or em_arr[word_list.index(n[j-1])][u_idx] == 0:\n",
    "                        pi_temp.append(float('-inf'))\n",
    "                    else:\n",
    "                        # append pi_value_v (current) = pi_value_u (previous) * transition_prob(u,v) * emission_prob(v,word)  \n",
    "                        pi_temp.append(pi_arr[u_idx_temp][j-1] + np.log(tr_arr[u_idx_temp+1][u_idx]) + np.log(em_arr[word_list.index(n[j-1])][u_idx]))\n",
    "                \n",
    "                #pi_value_v = max(_temp)\n",
    "                pi_arr[u_idx][j] = max(pi_temp) \n",
    "                \n",
    "        # column n+1 : STOP node: pi value = max(pi value of previous nodes * transition_prob)\n",
    "        elif j == len(n)+1:\n",
    "            pi_temp = []\n",
    "            for u_idx, u in enumerate((l_labels[1:-1])):\n",
    "                if tr_arr[u_idx+1][len(l_labels[1:-1])] == 0:\n",
    "                    pi_temp.append(float(\"-inf\"))\n",
    "                else:\n",
    "                    pi_temp.append(np.log(tr_arr[u_idx+1][len(l_labels[1:-1])]) + pi_arr[u_idx][j-1])\n",
    "            for u_idx_temp, u_temp in enumerate((l_labels[1:-1])):\n",
    "                pi_arr[u_idx_temp][j] = max(pi_temp)\n",
    "                \n",
    "    ### Backtracking Algo     \n",
    "    \n",
    "    # list to store predicted outputs\n",
    "    args = []\n",
    "    \n",
    "    # To store the index current node with the highes score\n",
    "    last_idx = len(l_labels[1:-1])\n",
    "    \n",
    "    # Iterating from n to 1: n, n-1, n-2...1\n",
    "    for i in range(len(n),0,-1):\n",
    "        \n",
    "        # array to store all temp scores calculated \n",
    "        temp = []\n",
    "        \n",
    "        # Iterating through the rows\n",
    "        for u_idx, u in enumerate((l_labels[1:-1])):\n",
    "            if tr_arr[u_idx+1][last_idx] == 0:\n",
    "                temp.append(float(\"-inf\"))\n",
    "            else:\n",
    "                # append the score = transition_prob * pi value to temp\n",
    "                temp.append(np.log(tr_arr[u_idx+1][last_idx]) + pi_arr[u_idx][i])\n",
    "                \n",
    "        # update last_idx with the index of the node that had the highest score\n",
    "        last_idx = np.argmax(temp)\n",
    "        \n",
    "        # append tag/label corresponding to node with highest score to output\n",
    "        args.append(l_labels[last_idx+1])\n",
    "\n",
    "    return list(reversed(args))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(word_list, labels_list):\n",
    "    weights = {}\n",
    "    list_em = []\n",
    "    list_tn = []\n",
    "    for i in labels_list[1:-1]:\n",
    "        for j in word_list:\n",
    "            em_tuple = (i,j)\n",
    "            list_em.append(em_tuple)\n",
    "    list_em.append(('STOP','none'))\n",
    "    for i in labels_list[0:-1]:\n",
    "        for j in labels_list[1:]:\n",
    "            tn_tuple = (i,j)\n",
    "            list_tn.append(tn_tuple)\n",
    "    for em in tqdm(list_em):\n",
    "        for tn in list_tn:\n",
    "            weights[(tn,em)] = 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "n (list)： list of input words to be predicted\n",
    "l_labels (str)： list of all possible states\n",
    "tr_arr (2d numpy array): transition table\n",
    "em_arr (2d numpy array): emission table\n",
    "word_list (list): list of words\n",
    "\n",
    "Output:\n",
    "args (list): list of predicted states \n",
    "\n",
    "Function:\n",
    "Takes input sentence of n words, outputs list of n predicted tags/states corresponding to input words \n",
    "'''\n",
    "def viterbi_sp(n, l_labels, tr_arr, em_arr, word_list, weights):\n",
    "        \n",
    "    # Initializoing pi array \n",
    "    pi_arr = np.zeros([len(l_labels[1:-1]),len(n)+2])\n",
    "    parent = np.zeros([len(l_labels[1:-1]),len(n)+2])\n",
    "    pred = []\n",
    "    \n",
    "    ### Forwarding Algo\n",
    "    \n",
    "    # Iterating through the columns of the pi array\n",
    "    for j in range(len(n)+2):\n",
    "        \n",
    "        # column 0: START nodes, assign value 1 to all nodes in column 0\n",
    "        if j == 0:\n",
    "            for i in range(len(pi_arr)):\n",
    "                pi_arr[i][j] = 0\n",
    "                \n",
    "        # column 1: nodes right after START, pi value = 1(pi value of START) * transition_prob * emission_ prob       \n",
    "        elif j == 1:\n",
    "            if n[j-1] not in word_list:\n",
    "                n[j-1] = \"#UNK#\"\n",
    "            for u_idx, u in enumerate((l_labels[1:-1])):\n",
    "                if tr_arr[0][u_idx] == 0 or em_arr[word_list.index(n[j-1])][u_idx] == 0:\n",
    "                    pi_arr[u_idx][j] = float('-inf')\n",
    "                else:\n",
    "                    pi_arr[u_idx][j] = weights[('START', l_labels[u_idx+1]),(l_labels[u_idx+1], n[j-1])]*(np.log(tr_arr[0][u_idx]) + np.log(em_arr[word_list.index(n[j-1])][u_idx]))\n",
    "                parent[u_idx][j] = 0\n",
    "        \n",
    "        # columns 2 to n : pi value = max(pi value of previous nodes * transition_prob * emission_ prob)\n",
    "        elif j > 1 and j < len(n)+1:\n",
    "            \n",
    "            # n[j-1]: current word in sentence, if not found in word_list, replace with \"#UNK#\"\n",
    "            if n[j-1] not in word_list:\n",
    "                n[j-1] = \"#UNK#\"\n",
    "            \n",
    "            # Iterating through column v: current column\n",
    "            for u_idx, u in enumerate((l_labels[1:-1])): # v\n",
    "                \n",
    "                # array to store temp scores\n",
    "                pi_temp = []\n",
    "                \n",
    "                # Iterating through column u: previous column\n",
    "                for u_idx_temp, u_temp in enumerate((l_labels[1:-1])): # u\n",
    "                    if tr_arr[u_idx_temp+1][u_idx] == 0 or em_arr[word_list.index(n[j-1])][u_idx] == 0:\n",
    "                        pi_temp.append(float('-inf'))\n",
    "                    else:\n",
    "                        # append pi_value_v (current) = pi_value_u (previous) * transition_prob(u,v) * emission_prob(v,word)  \n",
    "                        pi_temp.append(pi_arr[u_idx_temp][j-1] + \n",
    "                                       weights[(l_labels[u_idx_temp+1], l_labels[u_idx+1]),(l_labels[u_idx+1], n[j-1])]*\n",
    "                                       (np.log(tr_arr[u_idx_temp+1][u_idx]) + np.log(em_arr[word_list.index(n[j-1])][u_idx])))\n",
    "                \n",
    "                #pi_value_v = max(_temp)\n",
    "                pi_arr[u_idx][j] = max(pi_temp) \n",
    "                parent[u_idx][j] = np.argmax(pi_temp)+1\n",
    "                \n",
    "        # column n+1 : STOP node: pi value = max(pi value of previous nodes * transition_prob)\n",
    "        elif j == len(n)+1:\n",
    "            pi_temp = []\n",
    "            for u_idx, u in enumerate((l_labels[1:-1])):\n",
    "                if tr_arr[u_idx+1][len(l_labels[1:-1])] == 0:\n",
    "                    pi_temp.append(float(\"-inf\"))\n",
    "                else:\n",
    "                    pi_temp.append(weights[(l_labels[u_idx+1], 'STOP'),('STOP', 'none')]*\n",
    "                                   np.log(tr_arr[u_idx+1][len(l_labels[1:-1])]) + pi_arr[u_idx][j-1])\n",
    "            for u_idx_temp, u_temp in enumerate((l_labels[1:-1])):\n",
    "                pi_arr[u_idx_temp][j] = max(pi_temp)\n",
    "                parent[u_idx_temp][j] = np.argmax(pi_temp)+1\n",
    "                \n",
    "#     ### Backtracking Algo     \n",
    "    \n",
    "#     # list to store predicted outputs\n",
    "#     args = []\n",
    "    \n",
    "#     # To store the index current node with the highes score\n",
    "#     last_idx = len(l_labels[1:-1])\n",
    "    \n",
    "#     # Iterating from n to 1: n, n-1, n-2...1\n",
    "#     for i in range(len(n),0,-1):\n",
    "        \n",
    "#         # array to store all temp scores calculated \n",
    "#         temp = []\n",
    "        \n",
    "#         # Iterating through the rows\n",
    "#         for u_idx, u in enumerate((l_labels[1:-1])):\n",
    "#             if tr_arr[u_idx+1][last_idx] == 0:\n",
    "#                 temp.append(float(\"-inf\"))\n",
    "#             else:\n",
    "#                 # append the score = transition_prob * pi value to temp\n",
    "#                 temp.append(np.log(tr_arr[u_idx+1][last_idx]) + pi_arr[u_idx][i])\n",
    "                \n",
    "#         # update last_idx with the index of the node that had the highest score\n",
    "#         last_idx = np.argmax(temp)\n",
    "        \n",
    "#         # append tag/label corresponding to node with highest score to output\n",
    "#         args.append(l_labels[last_idx+1])\n",
    "\n",
    "#     return list(reversed(args))        \n",
    "                \n",
    "    ### Backtracking Algo     \n",
    "    \n",
    "    # list to store predicted outputs\n",
    "    args = []\n",
    "    \n",
    "    # To store the index current node with the highes score\n",
    "    last_idx = len(l_labels[1:-1])\n",
    "    \n",
    "    # Iterating from n to 1: n, n-1, n-2...1\n",
    "    \n",
    "    \n",
    "    for i in range(len(n)+1,1,-1):\n",
    "                \n",
    "        # update last_idx with the index of the node that had the highest score\n",
    "        if i == len(n)+1:\n",
    "            last_idx = int(parent[0][i])\n",
    "        else:\n",
    "            last_idx = int(parent[last_idx-1][i]) #-1 to undo the offset made when saving the argmax earlier\n",
    "        \n",
    "        # append tag/label corresponding to node with highest score to output\n",
    "        args.append(l_labels[int(last_idx)])\n",
    "\n",
    "    return list(reversed(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(data):\n",
    "    train_sen = []\n",
    "    train_labels = []\n",
    "    for i in data:\n",
    "        sentence_words = []\n",
    "        sentence_labels = []\n",
    "        for j in i:\n",
    "            word = j[0]\n",
    "            label = j[1]\n",
    "            sentence_words.append(word)\n",
    "            sentence_labels.append(label)\n",
    "        train_sen.append(sentence_words)\n",
    "        train_labels.append(sentence_labels)\n",
    "    return train_sen, train_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Part_5:\n",
    "    transition_table = []\n",
    "    emission_table = []\n",
    "    word_list = []\n",
    "    labels_list = []\n",
    "    weights_dict = {}\n",
    "    \n",
    "    \n",
    "    def __init__(self,file_path, weights = '', em_table = '', tr_table = ''):\n",
    "        words_en, labels_en = train(file_path)\n",
    "        self.labels_list = labels_en\n",
    "        x, y = clean(file_path)\n",
    "        self.word_list = generate_word_list(x)\n",
    "        tag_word_table = generate_table(x, y)\n",
    "\n",
    "        if tr_table == '':\n",
    "            print(\"Generating Transition Table\")\n",
    "            self.transition_table = generate_transition_table(words_en, labels_en)\n",
    "        else:\n",
    "            print(\"Initialising class with given transition table\")\n",
    "            self.transition_table = tr_table\n",
    "\n",
    "        if em_table == '':\n",
    "            print(\"Generating Emission Table\")\n",
    "            self.emission_table = generate_emission_table(self.word_list, tag_word_table)\n",
    "        else:\n",
    "            print(\"Initialising class with given emission table\")\n",
    "            self.emission_table = em_table\n",
    "        if weights == '':\n",
    "            print(\"Initialising Weights\")\n",
    "            self.weights_dict = init_weights(self.word_list, self.labels_list)\n",
    "        else:\n",
    "            print(\"Initialising class with given weights\")\n",
    "            self.weights_dict = weights\n",
    "\n",
    "\n",
    "    def get_weights_dict(self):\n",
    "        return self.weights_dict\n",
    "\n",
    "    def get_tr_table(self):\n",
    "        return self.transition_table\n",
    "\n",
    "    def get_em_table(self):\n",
    "        return self.emission_table\n",
    "    \n",
    "        \n",
    "\n",
    "    def get_features(self,key):\n",
    "        #Extract the states and word from the key\n",
    "        transition = key[0]\n",
    "        emission = key[1]\n",
    "        prev_state = transition[0]\n",
    "        current_state = transition[1]\n",
    "        word = emission[1]\n",
    "\n",
    "        #Retrieve the index of the states and word\n",
    "        prev_state_index = self.labels_list.index(prev_state)\n",
    "        #labels_list contains start, but current_state columns in both tr_table and em_table does not have start, so we have to minus 1 to offset the index to match that of the tables\n",
    "        current_state_index = self.labels_list.index(current_state) - 1 \n",
    "        word_index = self.word_list.index(word)\n",
    "\n",
    "        #Get the transition feature value\n",
    "        trans_val = self.transition_table[prev_state_index][current_state_index]\n",
    "\n",
    "        #Get the emission feature value\n",
    "        if current_state == \"STOP\": # if the state is STOP, there is no emission value\n",
    "            em_val = 0\n",
    "        else:\n",
    "            em_val = self.emission_table[word_index][current_state_index] \n",
    "\n",
    "        return trans_val + em_val\n",
    "        \n",
    "    def get_weights(self, key):\n",
    "        return self.weights_dict[key]\n",
    "    \n",
    "    def update_weights(self,new_weights, key):\n",
    "        self.weights_dict[key]  = new_weights\n",
    "\n",
    "\n",
    "\n",
    "    def perceptron_training(self,training_sentences, training_labels, epochs):\n",
    "        for iteration in range(epochs):\n",
    "            print(\"Epoch {}\".format(iteration))\n",
    "            for line,actual_labels in tqdm(zip(training_sentences, training_labels), total = len(training_sentences)):\n",
    "                pred_labels = viterbi_sp(line, self.labels_list, self.transition_table, self.emission_table,\n",
    "                                        self.word_list, self.weights_dict)\n",
    "                for i in range(len(pred_labels)):\n",
    "                    # Handle Start Case from state START transiting to the first State\n",
    "                    if i == 0:\n",
    "                        prev_state = 'START'\n",
    "                        actual_prev_state = 'START'\n",
    "\n",
    "                    # Handles the cases in the middle   \n",
    "                    else:\n",
    "                        prev_state = pred_labels[i-1]\n",
    "                        actual_prev_state = actual_labels[i-1]\n",
    "                    current_state = pred_labels[i]\n",
    "                    actual_current_state = actual_labels[i]\n",
    "                    if current_state != actual_current_state:\n",
    "                        word = line[i]\n",
    "                        weights = self.get_weights(((prev_state, current_state),(current_state, word)))\n",
    "                        pred_features = self.get_features(((prev_state, current_state), (current_state, word)))\n",
    "                        actual_features = self.get_features(((actual_prev_state, actual_current_state), (actual_current_state, word)))\n",
    "                        weights += (actual_features - pred_features)\n",
    "                        self.update_weights(weights, ((prev_state, current_state), (current_state, word)))\n",
    "\n",
    "#                 #Handle the last case that transits to STOP state\n",
    "#                 prev_state = pred_labels[-1]\n",
    "#                 actual_prev_state = actual_labels[-1]\n",
    "#                 current_state = 'STOP'\n",
    "#                 actual_current_state = 'STOP'\n",
    "#                 word = 'none'\n",
    "#                 weights = self.get_weights(((prev_state,current_state), (current_state, word)))\n",
    "#                 pred_features = self.get_features(((prev_state, current_state), (current_state, word)))\n",
    "#                 actual_features = self.get_features(((prev_state,current_state), (current_state, word)))\n",
    "#                 weights += (actual_features - pred_features)\n",
    "#                 self.update_weights(weights, ((prev_state, current_state), (current_state, word)))\n",
    "            \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "189291it [00:00, 1240500.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 181628/181628 [00:07<00:00, 23212.23it/s]\n",
      "181628it [00:00, 2396232.50it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Transition Table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [00:06,  3.39it/s]\n",
      "14it [00:00, 136.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Emission Table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18213it [02:10, 139.97it/s]\n",
      "  0%|▏                                                                          | 723/382474 [00:00<00:58, 6530.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising Weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 382474/382474 [01:49<00:00, 3483.11it/s]\n"
     ]
    }
   ],
   "source": [
    "part5 = Part_5(\"en/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part5_em_table = part5.get_em_table()\n",
    "# part5_tr_table = part5.get_tr_table()\n",
    "# part5_weights = part5.get_weights_dict()\n",
    "# part5 = Part_5(\"en/train\", weights = part5_weights, em_table = part5_em_table, tr_table = part5_tr_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_en, labels_en = train(\"en/train\")\n",
    "sentences, labels = split_train(words_en)\n",
    "# print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sentence = sentences[0]\n",
    "# yx = viterbi_sp(test_sentence, part5.labels_list, part5.transition_table, part5.emission_table, part5.word_list, part5.weights_dict)\n",
    "# bobby = viterbi(test_sentence, part5.labels_list, part5.transition_table, part5.emission_table, part5.word_list)\n",
    "\n",
    "# print(yx)\n",
    "# print(\"\\n\")\n",
    "# print(bobby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 5/7663 [00:00<02:49, 45.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7663/7663 [22:06<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "part5.perceptron_training(sentences, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(part5.weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "updated_weights = part5.weights_dict\n",
    "with open(\"part5_weights.json\", 'wb') as f:\n",
    "    pickle.dump(updated_weights, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inputs:\n",
    "input_file_path (str): path to input file\n",
    "output_file_path (str): path to output file\n",
    "labels (list): list of all possible tags/states\n",
    "transition_table (2d numpy array): transition table calculated from training set\n",
    "emission_table (2d numpy array): emission table calculated from training set\n",
    "word_list (list): list of words\n",
    "\n",
    "Output:\n",
    " - : write predicted results to output file\n",
    "\n",
    "Function:\n",
    "Takes input file, for every sentence in input file, generates the predicted tag/state for each word in sentence by running\n",
    "viterbi() and write to output file \n",
    "'''\n",
    "def generate_prediction_sp(input_file_path, output_file_path, labels, transition_table, emission_table, word_list, weights):\n",
    "    with open (input_file_path, 'r', encoding=\"utf-8\") as dev_in: \n",
    "        lines = dev_in.readlines()\n",
    "        words = []\n",
    "\n",
    "        temp = []\n",
    "        for l in lines: \n",
    "            if l != \"\\n\":\n",
    "                l_split = l.strip().split(\" \")\n",
    "                la = l_split[-1]\n",
    "                temp.append(la)\n",
    "            else: \n",
    "                words.append(temp)\n",
    "                temp = []\n",
    "\n",
    "        words_copy = copy.deepcopy(words)\n",
    "        labels_out = []\n",
    "        for sentence in tqdm(words):\n",
    "            label_out = viterbi_sp(sentence, labels, transition_table, emission_table, word_list, weights )\n",
    "            labels_out.append(label_out)\n",
    "            \n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for i in tqdm(range(len(words_copy))):\n",
    "            for j in range(len(words_copy[i])):\n",
    "                output = words_copy[i][j] + \" \" + labels_out[i][j] + \"\\n\"\n",
    "                fout.write(output)\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1094/1094 [05:55<00:00,  3.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 1094/1094 [00:00<00:00, 45778.56it/s]\n"
     ]
    }
   ],
   "source": [
    "generate_prediction_sp(\"en/dev.in\", \"en_sp.out\", part5.labels_list, part5.transition_table, part5.emission_table, part5.word_list, part5.weights_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
